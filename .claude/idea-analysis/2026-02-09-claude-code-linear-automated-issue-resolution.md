---
source: https://youtu.be/jsI18Htgf8k
date: 2026-02-09
speaker: Unknown (indie dev / YouTuber, runs production messaging app for sales agents)
type: idea-analysis
verdict: Steal one piece
viability: green
hype-risk: yellow
business-fit: green
segment-fit: A
tags: [idea-analysis, developer-workflow, claude-code, automation, linear, sub-agents]
---

# Claude Code + Linear: Automated Issue Resolution Workflow

## TL;DR

Workflow combining Claude Code slash commands with Linear (project management) to automate the full issue-resolution lifecycle â€” from fetching an issue to planning via TDD, implementing, opening a PR, running code review, fixing review issues, and updating Linear â€” all with a single slash command. Only manual step: initial planning/decision-making.

## Scores

- **Idea Viability** ðŸŸ¢ **Strong** â€” Real productivity workflow with demonstrated results; not a product to sell but a process to adopt.
- **Hype/BS** ðŸŸ¡ **Medium risk** â€” Practitioner with real product, but newsletter funnel + cherry-picked demo + "hours saved" framing need scrutiny.
- **dotslash.dev Fit** ðŸŸ¢ **Strong (Segment A)** â€” Directly applicable to own dev workflow AND packageable as consulting IP for tech teams.
- **Effort vs Payoff** ðŸŸ¢ **High payoff** â€” Already have 80% of this built; marginal effort to formalize the remaining pieces.

**One-liner: "You're already doing this. Steal the Linear MCP integration and the iterative review loop pattern â€” skip the newsletter."**

## The Workflow (Detailed)

### Process Diagram

```
Plan (human, 5-10 min)
  â†’ Fetch issue via MCP
  â†’ Create branch (auto-linked to Linear via naming convention)
  â†’ Spin up plan agent (TDD â€” tests first)
  â†’ Spin up implementation agent
  â†’ Commit / push / open PR
  â†’ Monitor PR checks (background task)
  â†’ PR review (sub-agent)
  â†’ Fix review issues (sub-agent)
  â†’ Re-review until approved
  â†’ Update Linear issue with findings
  â†’ Human reviews final PR and merges
```

### Key Design Decisions

1. **Sub-agents for everything after planning** â€” keeps main context window small. Demo used ~73K tokens over 26 minutes with plenty of context left.
2. **TDD-first in the plan agent** â€” writes tests before implementation, ensuring coverage before review catches remaining issues.
3. **Branch naming convention** â€” includes Linear issue ID, auto-links and moves issues through statuses (backlog â†’ in progress â†’ in review).
4. **Iterative review loop** â€” review agent flags issues â†’ fix agent addresses them â†’ re-review validates â†’ repeat until approved.
5. **Background CI monitoring** â€” agent watches for GitHub Actions check failures and auto-fixes.

### Linear Organization

- **Issues** = atomic unit of work, narrow enough for one context window
- **Projects** = groups of related issues (e.g., "implement payments" = Stripe + DB + pricing page + checkout)
- **Milestones** = optional grouping within projects
- **Blocking** = Linear natively supports issue dependencies; Claude uses this for execution ordering

### Slash Commands Mentioned

1. **Create Linear issue** â€” template-based, walks through refinement until well-defined with no ambiguity. Includes: summary, current vs expected behavior, acceptance criteria, scope, root cause analysis.
2. **Create Linear project** â€” 6-step workflow, creates individual issues via the issue slash command, sets up blocking relationships.
3. **Resolve issue** â€” the main automation. Single command that executes the full pipeline above.
4. **Enhance issue** â€” for issues created from other sources (e.g., Sentry), normalizes them to the standard template.

## Viability Analysis

### Why it works

- Solves real pain point every developer using AI coding tools faces (babysitting through repetitive steps)
- The steps after planning ARE deterministic and repeatable â€” good automation target
- Sub-agent pattern for context preservation is genuinely clever and matches Anthropic's own recommendations
- Demo is verifiable â€” real feature (read receipts) deployed to real production app
- Linear's blocking/dependency system provides real structural advantage over ad-hoc task management
- 26-min / 73K-token result is concrete and plausible for a well-scoped issue

### Limitations not discussed

- No mention of failure rate â€” how often does the full pipeline actually produce a shippable PR?
- Race condition caught in review is presented as "the system works" but also reveals AI-generated code quality concerns
- Context window management with sub-agents means each sub-agent starts with limited context about the broader system
- Works best for well-scoped, isolated issues â€” complex cross-cutting changes would likely fail
- TDD-first sounds good but AI-generated tests often test the implementation rather than the behavior

## Hype/BS Detailed Assessment

### Green Flags âœ…

- **Practitioner** â€” has actual production messaging app, not just teaching
- **Shows real code** â€” real GitHub activity, real deployment, real PR with real review comments
- **Intellectually honest about planning** â€” "no one knows your domain better than you, including any AI"
- **Shows warts** â€” race condition caught in review, 5 issues found, doesn't pretend it's flawless
- **Concrete numbers** â€” 26 minutes, 73K tokens, 5 review issues

### Red Flags ðŸš©

- **Newsletter funnel** â€” free Substack "The AI Launchpad" with prompts as the hook. Classic content-to-newsletter pipeline.
- **Cherry-picked demo** â€” showed ONE successful run. No mention of failure rate, hallucinations, context window blowups, or issues the review loop didn't catch.
- **"Saving me hours every day"** â€” vague time savings without baseline. How many issues/day? What's actual error rate?
- **GitHub contribution graph as proof** â€” correlation â‰  causation. More commits â‰  better software.
- **"Next step: REPL loops / multi-bot"** â€” teases autonomous coding without acknowledging massive reliability gap between supervised and unsupervised loops.
- **"Anyone can do this" implied** â€” prompts shared on newsletter, but the effectiveness depends heavily on codebase quality, test infrastructure, CI setup, and well-written issues.

### Verdict

Genuine practitioner with real insights, but packaging it for content/newsletter growth. The workflow is real; the implied ease is exaggerated.

## dotslash.dev Fit Analysis

### Segment A (AI Consulting) â€” ðŸŸ¢ Strong

- Directly relevant to consulting offering: "here's how we set up automated dev workflows for your team" is a tangible deliverable
- Already have slash commands (`/finalize`, `/review-pr`), sub-agent patterns, and TDD discipline â€” just need to formalize the end-to-end chain
- Packageable as "developer productivity audit" or "AI workflow setup" engagement for tech teams
- Linear MCP integration is the one missing piece â€” worth exploring for consulting clients using Linear
- Demonstrates production AI expertise (one of the consulting differentiators)

### Segment B (SMB Product) â€” ðŸ”´ Irrelevant

- Copenhagen plumbers don't need automated PR review loops
- Pure developer tooling, no SMB application

## Effort vs Payoff

### Already have (80%)

- âœ… Slash commands for commit, PR, review (`/finalize`)
- âœ… Sub-agent patterns (documented in CLAUDE.md)
- âœ… TDD discipline (jqwik property tests, test-first architecture)
- âœ… PR review automation (`/review-pr`)
- âœ… Atlassian MCP tools in skill list (`atlassian:triage-issue`, etc.)

### Would need to add (20%)

- Linear/Jira MCP integration for issue fetching + status updates
- Single "resolve issue" slash command chaining existing commands
- Background task monitoring for CI checks
- Branch naming convention automation tied to issue IDs

### Estimated effort

~1 day to formalize into a `/resolve-issue` skill that chains existing components. ROI is in own shipping velocity + consulting IP.

## What to Steal

1. **The "resolve issue" orchestrator pattern** â€” one slash command chaining: fetch issue â†’ branch â†’ plan â†’ implement â†’ commit â†’ PR â†’ review â†’ fix â†’ re-review. Have all pieces; chain them.
2. **Linear/Jira MCP for status automation** â€” branch naming conventions that auto-move issues through statuses.
3. **Background CI monitoring** â€” agent watches for check failures and auto-fixes as addition to `/finalize` flow.
4. **Issue template discipline** â€” issues written to be agent-resolvable without clarification. Acceptance criteria, scope boundaries, root cause analysis baked in.

## What to Ignore

- The newsletter and prompt sharing â€” own prompts are already more sophisticated (architecture rules, property testing, Either/Validation library are deeper)
- "REPL loops / multi-bot" teaser â€” vaporware until proven. Unsupervised agent loops have terrible reliability at scale.
- GitHub contribution graph flexing
- The specific Linear choice â€” same pattern works with Jira, GitHub Issues, or any issue tracker with MCP/API access

## Actionable Next Steps

1. Build a `/resolve-issue` skill that calls `/finalize` as its final step
2. Integrate Atlassian MCP (already available) for issue fetching and status updates
3. Add iterative review loop to the pipeline (review â†’ fix â†’ re-review until clean)
4. Package the full workflow as a consulting deliverable: "AI-automated dev pipeline setup"
